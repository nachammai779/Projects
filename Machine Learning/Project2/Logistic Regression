Logistic regression program
"""
This program performs two different logistic regression implementations a dataset
of the format [int,boolean], one implementation is in this file and one from the sklearn library.
The objective of second implementation is just to verify how well my code performs against scikit learn library.
"""
 import math
import numpy as np
import pandas as pd
import time
from pandas import DataFrame
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
 
df = pd.read_csv("data.csv", header=0)
 # clean up data
df.columns = ["X","Y"]
print(df)
 # formats the input data into two arrays, one of independant variables
# and one of the dependant variable
X = df[["X"]]
X = np.array(X)
 Y = df["Y"].map(lambda x: float(x.rstrip(';')))
Y = np.array(Y)
 print(X)
print(Y)
 
## Time scikit learn model
elap_time1=time.perf_counter_ns()
cpu_time1=time.process_time_ns()
 clf = LogisticRegression()
clf.fit(X,Y)
 elap_time2=time.perf_counter_ns()
cpu_time2=time.process_time_ns()
 print ('score Scikit learn: ', clf.score(X,Y))
print ('elapsed time for Scikit learn in ns: ', (elap_time2 - elap_time1))
print ('processed time for Scikit learn in ns: ', (cpu_time2 - cpu_time1))
 
##The sigmoid function
def Sigmoid(z):
        G_of_Z = float(1.0 / float((1.0 + math.exp(-1.0*z))))
        return G_of_Z
 
##The hypothesis is the linear combination of all the known factors x[i] and their current estimated coefficients theta[i]
def Hypothesis(theta, x):
        z = 0
        for i in range(len(theta)):
                z += x[i]*theta[i]
        return Sigmoid(z)
 
##The Log likelihood function
def LL_Function(X,Y,theta,m):
        sumOfErrors = 0
        for i in range(m):
                xi = X[i]
                hi = Hypothesis(theta,xi)
                if Y[i] == 1:
                        error = Y[i] * math.log(hi)
                elif Y[i] == 0:
                        error = (1-Y[i]) * math.log(1-hi)
                sumOfErrors += error
        const = -1/m
        J = const * sumOfErrors
        return J
 
##This function creates the gradient component for each Theta value
def LL_Function_Derivative(X,Y,theta,j,m,alpha):
        sumErrors = 0
        for i in range(m):
                xi = X[i]
                xij = xi[j]
                hi = Hypothesis(theta,X[i])
                error = (hi - Y[i])*xij
                sumErrors += error
        m = len(Y)
        constant = float(alpha)/float(m)
        J = constant * sumErrors
        return J
 
##For each theta, the partial differential
##The gradient, or vector from the current point in Theta-space (each theta value is its own dimension) to the more accurate point,
##is the vector with each dimensional component being the partial differential for each theta value
def Gradient_Descent(X,Y,theta,m,alpha):
        new_theta = []
#        constant = alpha/m
        for j in range(len(theta)):
                LLDerivative = LL_Function_Derivative(X,Y,theta,j,m,alpha)
                new_theta_value = theta[j] - LLDerivative
                new_theta.append(new_theta_value)
        return new_theta
 ##The high level function for the LR algorithm which, for a number of steps (num_iters) finds gradients which take
##the Theta values (coefficients of known factors) from an estimation closer (new_theta) to their "optimum estimation" which is the
##set of values best representing the system in a linear combination model
def Logistic_Regression(X,Y,alpha,theta,num_iters):
        m = len(Y)
        for x in range(num_iters):
                new_theta = Gradient_Descent(X,Y,theta,m,alpha)
                theta = new_theta

#                if x % 1 == 0:
                        #here the cost function is used to present the final hypothesis of the model in the same form for each gradient-step iteration
#                        print (x)
#                        print ('theta ', theta)
#                        print ('log likelihood is ', LL_Function(X,Y,theta,m))
 
       Declare_Winner(theta)
 ##This method compares the accuracy of the model generated by the scikit library with the model generated by this implementation
def Declare_Winner(theta):
        score = 0
        winner = ""
        #first scikit LR is tested for each independent var in the dataset and its prediction is compared against the dependent var
        #if the prediction is the same as the dataset measured value it counts as a point for the scikit version of LR
#       scikit_score = clf.score(X,Y)
        length = len(X)
        for i in range(length):
#                prediction = round(Hypothesis(X[i],theta))
                prediction = round(Hypothesis(theta,X[i]))
#                print('theta:', theta)
#                print('X[i]', X[i])
                print('Hypothesis(theta,X[i])', Hypothesis(theta,X[i]))
#                print('prediction:', prediction)
                answer = Y[i]
                if prediction == answer:
                        score += 1
        #the same process is repeated for the implementation from this module and the scores compared to find the higher match-rate
#        print(score)
#        print(length)
        my_score = float(score) / float(length)
        print ('Your score: ', my_score)
 # These are the initial guesses for theta as well as the learning rate of the algorithm
# A learning rate too low will not close in on the most accurate values within a reasonable number of iterations
# An alpha too high might overshoot the accurate values or cause erratic guesses
# Each iteration increases model accuracy
initial_theta = [0]
alpha = 0.1
iterations = 100
 
# Make sure to comment the print statements inside the Logistic_Regression function if your are planning to use the timer values
elap_time3=time.perf_counter_ns()
cpu_time3=time.process_time_ns()
Logistic_Regression(X,Y,alpha,initial_theta,iterations)
 elap_time4=time.perf_counter_ns()
cpu_time4=time.process_time_ns()
 
print ('elapsed time for your learn in ns: ', (elap_time4 - elap_time3))
print ('processed time for your learn in ns: ', (cpu_time4 - cpu_time3))
